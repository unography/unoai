{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp data.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from unoai.imports import *\n",
    "from unoai.data.load import *\n",
    "from unoai.vision.augmentations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_cifar10(ds_dir: str, batch_size: int = None,\n",
    "                normalize: bool = False, padding: int = None, augmentation: str = None):\n",
    "    if not os.path.exists(ds_dir):\n",
    "        os.makedirs(ds_dir)\n",
    "\n",
    "    img_sz = (32, 32)\n",
    "    fname_base = \"cifar10\"\n",
    "    if normalize:\n",
    "        fname_base += \"-normalized\"\n",
    "    if padding is not None:\n",
    "        fname_base += f\"-{padding}-pad\"\n",
    "    if augmentation is not None:\n",
    "        fname_base += f\"-{augmentation}-aug\"\n",
    "\n",
    "    train_path = os.path.join(ds_dir, f\"{fname_base}.train.tfrecords\")\n",
    "    test_path = os.path.join(ds_dir, f\"{fname_base}.test.tfrecords\")\n",
    "\n",
    "    def parser_train(tf_example):\n",
    "        if padding is not None:\n",
    "            train_img_sz = (32+padding*2, 32+padding*2)\n",
    "        else:\n",
    "            train_img_sz = img_sz\n",
    "        x, y = parse_tf_example_img(tf_example, train_img_sz[0], train_img_sz[1])\n",
    "        if padding is not None:\n",
    "            x = tf.image.random_flip_left_right(tf.image.random_crop(x, [img_sz[0], img_sz[1], 3]))\n",
    "        if augmentation and augmentation == 'cutout':\n",
    "            x = cutout(x,h=8,w=8)\n",
    "        return x, y\n",
    "\n",
    "    def parser_test(tf_example):\n",
    "        x, y = parse_tf_example_img(tf_example, img_sz[0], img_sz[1])\n",
    "        return x, y\n",
    "\n",
    "    if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        if normalize:\n",
    "            X_train_mean = np.mean(X_train, axis=(0,1,2))\n",
    "            X_train_std = np.std(X_train, axis=(0,1,2))\n",
    "            X_train = (X_train - X_train_mean) / X_train_std\n",
    "            X_test = (X_test - X_train_mean) / X_train_std\n",
    "        if padding is not None:\n",
    "            pad_fn = lambda x: np.pad(x, [(0, 0), (padding, padding), (padding, padding), (0, 0)], mode='reflect')\n",
    "            X_train = pad_fn(X_train)\n",
    "\n",
    "        store_np_imgs_as_tfrecord(train_path, X_train, y_train)\n",
    "        store_np_imgs_as_tfrecord(test_path, X_test, y_test)\n",
    "\n",
    "    num_train = 50000\n",
    "\n",
    "    train = read_tfrecord_as_dataset(ds_path=train_path, parser=parser_train, batch_size=batch_size, shuffle=True, shuffle_buffer_size=num_train)\n",
    "    test = read_tfrecord_as_dataset(ds_path=test_path, parser=parser_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train, test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)â€¦",
   "language": "python",
   "name": "venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
